{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kdeo6piwLOOb"
      },
      "source": [
        "# Mount Drive, Installs and Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kGQk5Gevain6",
        "outputId": "ad9aa208-648b-4933-e3e7-51a1fd1fcc96"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_RUcRPhvLOi8",
        "outputId": "e78008aa-cc04-4f45-e638-bf5cb4d70588"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: biopython in /usr/local/lib/python3.11/dist-packages (1.85)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from biopython) (2.0.2)\n",
            "Requirement already satisfied: pytorch_wavelets in /usr/local/lib/python3.11/dist-packages (1.3.0)\n",
            "Requirement already satisfied: pywavelets in /usr/local/lib/python3.11/dist-packages (1.8.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from pytorch_wavelets) (2.0.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from pytorch_wavelets) (1.17.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from pytorch_wavelets) (2.6.0+cu124)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch->pytorch_wavelets) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch->pytorch_wavelets) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->pytorch_wavelets) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->pytorch_wavelets) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->pytorch_wavelets) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->pytorch_wavelets) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->pytorch_wavelets) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->pytorch_wavelets) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch->pytorch_wavelets) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch->pytorch_wavelets) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch->pytorch_wavelets) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch->pytorch_wavelets) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch->pytorch_wavelets) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch->pytorch_wavelets) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch->pytorch_wavelets) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->pytorch_wavelets) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->pytorch_wavelets) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->pytorch_wavelets) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch->pytorch_wavelets) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->pytorch_wavelets) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->pytorch_wavelets) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->pytorch_wavelets) (3.0.2)\n",
            "Requirement already satisfied: anndata in /usr/local/lib/python3.11/dist-packages (0.11.4)\n",
            "Requirement already satisfied: array-api-compat!=1.5,>1.4 in /usr/local/lib/python3.11/dist-packages (from anndata) (1.11.2)\n",
            "Requirement already satisfied: h5py>=3.7 in /usr/local/lib/python3.11/dist-packages (from anndata) (3.13.0)\n",
            "Requirement already satisfied: natsort in /usr/local/lib/python3.11/dist-packages (from anndata) (8.4.0)\n",
            "Requirement already satisfied: numpy>=1.23 in /usr/local/lib/python3.11/dist-packages (from anndata) (2.0.2)\n",
            "Requirement already satisfied: packaging>=24.2 in /usr/local/lib/python3.11/dist-packages (from anndata) (24.2)\n",
            "Requirement already satisfied: pandas!=2.1.0rc0,!=2.1.2,>=1.4 in /usr/local/lib/python3.11/dist-packages (from anndata) (2.2.2)\n",
            "Requirement already satisfied: scipy>1.8 in /usr/local/lib/python3.11/dist-packages (from anndata) (1.14.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas!=2.1.0rc0,!=2.1.2,>=1.4->anndata) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas!=2.1.0rc0,!=2.1.2,>=1.4->anndata) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas!=2.1.0rc0,!=2.1.2,>=1.4->anndata) (2025.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas!=2.1.0rc0,!=2.1.2,>=1.4->anndata) (1.17.0)\n",
            "Requirement already satisfied: scanpy in /usr/local/lib/python3.11/dist-packages (1.11.0)\n",
            "Requirement already satisfied: anndata>=0.8 in /usr/local/lib/python3.11/dist-packages (from scanpy) (0.11.4)\n",
            "Requirement already satisfied: h5py>=3.7 in /usr/local/lib/python3.11/dist-packages (from scanpy) (3.13.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from scanpy) (1.4.2)\n",
            "Requirement already satisfied: legacy-api-wrap>=1.4 in /usr/local/lib/python3.11/dist-packages (from scanpy) (1.4.1)\n",
            "Requirement already satisfied: matplotlib>=3.6 in /usr/local/lib/python3.11/dist-packages (from scanpy) (3.10.0)\n",
            "Requirement already satisfied: natsort in /usr/local/lib/python3.11/dist-packages (from scanpy) (8.4.0)\n",
            "Requirement already satisfied: networkx>=2.7 in /usr/local/lib/python3.11/dist-packages (from scanpy) (3.4.2)\n",
            "Requirement already satisfied: numba>=0.57 in /usr/local/lib/python3.11/dist-packages (from scanpy) (0.60.0)\n",
            "Requirement already satisfied: numpy>=1.24 in /usr/local/lib/python3.11/dist-packages (from scanpy) (2.0.2)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.11/dist-packages (from scanpy) (24.2)\n",
            "Requirement already satisfied: pandas>=1.5 in /usr/local/lib/python3.11/dist-packages (from scanpy) (2.2.2)\n",
            "Requirement already satisfied: patsy!=1.0.0 in /usr/local/lib/python3.11/dist-packages (from scanpy) (1.0.1)\n",
            "Requirement already satisfied: pynndescent>=0.5 in /usr/local/lib/python3.11/dist-packages (from scanpy) (0.5.13)\n",
            "Requirement already satisfied: scikit-learn<1.6.0,>=1.1 in /usr/local/lib/python3.11/dist-packages (from scanpy) (1.5.2)\n",
            "Requirement already satisfied: scipy>=1.8 in /usr/local/lib/python3.11/dist-packages (from scanpy) (1.14.1)\n",
            "Requirement already satisfied: seaborn>=0.13 in /usr/local/lib/python3.11/dist-packages (from scanpy) (0.13.2)\n",
            "Requirement already satisfied: session-info2 in /usr/local/lib/python3.11/dist-packages (from scanpy) (0.1.2)\n",
            "Requirement already satisfied: statsmodels>=0.13 in /usr/local/lib/python3.11/dist-packages (from scanpy) (0.14.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from scanpy) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from scanpy) (4.12.2)\n",
            "Requirement already satisfied: umap-learn!=0.5.0,>=0.5 in /usr/local/lib/python3.11/dist-packages (from scanpy) (0.5.7)\n",
            "Requirement already satisfied: array-api-compat!=1.5,>1.4 in /usr/local/lib/python3.11/dist-packages (from anndata>=0.8->scanpy) (1.11.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.6->scanpy) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.6->scanpy) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.6->scanpy) (4.56.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.6->scanpy) (1.4.8)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.6->scanpy) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.6->scanpy) (3.2.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.6->scanpy) (2.8.2)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba>=0.57->scanpy) (0.43.0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.5->scanpy) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.5->scanpy) (2025.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn<1.6.0,>=1.1->scanpy) (3.6.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib>=3.6->scanpy) (1.17.0)\n",
            "Requirement already satisfied: igraph in /usr/local/lib/python3.11/dist-packages (0.11.8)\n",
            "Requirement already satisfied: texttable>=1.6.2 in /usr/local/lib/python3.11/dist-packages (from igraph) (1.7.0)\n",
            "Requirement already satisfied: leidenalg in /usr/local/lib/python3.11/dist-packages (0.10.2)\n",
            "Requirement already satisfied: igraph<0.12,>=0.10.0 in /usr/local/lib/python3.11/dist-packages (from leidenalg) (0.11.8)\n",
            "Requirement already satisfied: texttable>=1.6.2 in /usr/local/lib/python3.11/dist-packages (from igraph<0.12,>=0.10.0->leidenalg) (1.7.0)\n",
            "Requirement already satisfied: louvain in /usr/local/lib/python3.11/dist-packages (0.8.2)\n",
            "Requirement already satisfied: igraph<0.12,>=0.10.0 in /usr/local/lib/python3.11/dist-packages (from louvain) (0.11.8)\n",
            "Requirement already satisfied: texttable>=1.6.2 in /usr/local/lib/python3.11/dist-packages (from igraph<0.12,>=0.10.0->louvain) (1.7.0)\n",
            "The tensorboard extension is already loaded. To reload it, use:\n",
            "  %reload_ext tensorboard\n"
          ]
        }
      ],
      "source": [
        "!pip install biopython\n",
        "!pip install pytorch_wavelets pywavelets\n",
        "!pip install anndata\n",
        "!pip install scanpy\n",
        "!pip install igraph\n",
        "!pip install leidenalg\n",
        "!pip install louvain\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pywt\n",
        "from pytorch_wavelets import DWTForward, DWTInverse\n",
        "from pytorch_wavelets import DTCWTForward, DTCWTInverse\n",
        "import matplotlib.pyplot as plt\n",
        "import json\n",
        "import logging\n",
        "from tqdm import tqdm\n",
        "from Bio import SeqIO\n",
        "import traceback\n",
        "import gc\n",
        "import re\n",
        "import tqdm\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from collections import defaultdict\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.nn.utils import spectral_norm\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "import psutil\n",
        "import anndata as ad\n",
        "import scanpy as sc\n",
        "import scipy.sparse as sp\n",
        "from torch.amp import GradScaler, autocast\n",
        "import umap\n",
        "import seaborn as sns\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "from scipy.spatial.distance import cdist\n",
        "\n",
        "import umap\n",
        "from sklearn.metrics import adjusted_rand_score\n",
        "\n",
        "from sklearn.utils import resample\n",
        "\n",
        "%load_ext tensorboard"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gvzDIhCg-Sla"
      },
      "source": [
        "# Sarscov2 S Protein"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PSlosa6ZE7tx"
      },
      "source": [
        "## (ONLY RUN ONCE - IF SOMEONE ALREADY DOWNLOADED THE DATA, DO NOT RUN THIS AGAIN) Download from NCBI and Save to Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z4yQ9huv3f_O"
      },
      "outputs": [],
      "source": [
        "# (species: 12,620 total genomes) Severe acute respiratory syndrome-related coronavirus\n",
        "'''https://www.ncbi.nlm.nih.gov/datasets/taxonomy/694009/'''\n",
        "\n",
        "# Set the path where you want to save the data\n",
        "data_dir = '/content/drive/MyDrive/540data/sarscov2'\n",
        "\n",
        "# Create the directory if it doesn't exist\n",
        "if not os.path.exists(data_dir):\n",
        "    os.makedirs(data_dir)\n",
        "\n",
        "# Change to the data directory\n",
        "%cd $data_dir\n",
        "\n",
        "# Download NCBI Datasets command-line tool\n",
        "!curl -o datasets 'https://ftp.ncbi.nlm.nih.gov/pub/datasets/command-line/v2/linux-amd64/datasets'\n",
        "!curl -o dataformat 'https://ftp.ncbi.nlm.nih.gov/pub/datasets/command-line/v2/linux-amd64/dataformat'\n",
        "\n",
        "# Make executable\n",
        "!chmod +x datasets dataformat\n",
        "\n",
        "# Download data - specifically including CDS for spike protein\n",
        "!./datasets download virus protein S taxon 694009 --host human --include cds --filename spikes.zip\n",
        "\n",
        "# Create directories for extracted data\n",
        "!mkdir -p spike_data\n",
        "!mkdir -p cds_sequences\n",
        "\n",
        "# Unzip all files to examine structure\n",
        "!unzip spikes.zip -d spike_data\n",
        "\n",
        "# List the contents of the zip file\n",
        "!unzip -l spikes.zip\n",
        "\n",
        "# Extract metadata\n",
        "!./dataformat tsv virus-genome --inputfile spike_data/ncbi_dataset/data/data_report.jsonl --fields accession,virus-name,host-name,isolate-collection-date > spike_cds_metadata.tsv\n",
        "\n",
        "# Copy the CDS files to the dedicated directory\n",
        "!cp spike_data/ncbi_dataset/data/cds.fna cds_sequences/spike_cds.fna"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Na4-h8NtLxsM"
      },
      "source": [
        "## Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "5LhAzedaQtvL"
      },
      "outputs": [],
      "source": [
        "def read_jsonl(file_path):\n",
        "    \"\"\"Reads metadata from JSONL file.\"\"\"\n",
        "    with open(file_path, 'r') as f:\n",
        "        for line in f:\n",
        "            yield json.loads(line)\n",
        "\n",
        "def parse_cds_file(cds_file, batch_size=10000):\n",
        "    \"\"\"Parse the CDS file and return a dictionary of sequences with isolate information.\"\"\"\n",
        "    sequences = {}\n",
        "    isolates = {}\n",
        "\n",
        "    # Process in smaller batches to manage memory\n",
        "    batch_count = 0\n",
        "\n",
        "    for record in tqdm(SeqIO.parse(cds_file, \"fasta\"), desc=\"Parsing CDS file\"):\n",
        "        accession = record.id.split('.')[0]\n",
        "        sequences[accession] = str(record.seq)\n",
        "\n",
        "        # Extract isolate information from the description\n",
        "        # Example: [isolate=SARS-CoV-2/human/USA/UT-UPHL-250228674407/2025]\n",
        "        isolate_match = re.search(r'\\[isolate=(.*?)\\]', record.description)\n",
        "        if isolate_match:\n",
        "            isolates[accession] = isolate_match.group(1)\n",
        "        else:\n",
        "            isolates[accession] = \"Unknown\"\n",
        "\n",
        "        batch_count += 1\n",
        "        if batch_count % batch_size == 0:\n",
        "            gc.collect()\n",
        "\n",
        "    return sequences, isolates\n",
        "\n",
        "def extract_variant_info(entry):\n",
        "    \"\"\"Extract variant information from taxonomy/lineage data.\"\"\"\n",
        "    variant = \"Unknown\"\n",
        "\n",
        "    if 'virus' in entry and 'lineage' in entry['virus']:\n",
        "        lineage = entry['virus']['lineage']\n",
        "\n",
        "        # Extract variant name from the taxonomy list\n",
        "        if isinstance(lineage, list) and lineage:\n",
        "            # Get the most specific taxon (last in list)\n",
        "            last_item = lineage[-1]\n",
        "            if isinstance(last_item, dict) and 'name' in last_item:\n",
        "                variant = last_item['name']\n",
        "            else:\n",
        "                variant = str(last_item)\n",
        "        else:\n",
        "            variant = str(lineage)\n",
        "\n",
        "    # Get other metadata (there is more if we want): https://www.ncbi.nlm.nih.gov/datasets/docs/v2/reference-docs/data-reports/virus/\n",
        "    collection_date = entry.get('collection_date', 'Unknown')\n",
        "    geo_location = entry.get('geo_location', 'Unknown')\n",
        "\n",
        "    return variant, collection_date, geo_location\n",
        "\n",
        "def process_data(cds_file, metadata_file, top_n=5, samples_per_variant=100, chunk_size=1000):\n",
        "    \"\"\"\n",
        "    Efficiently process CDS and metadata files to create a filtered AnnData object.\n",
        "    Only processes sequences from top variants and limits samples per variant.\n",
        "    \"\"\"\n",
        "    # Initialize counters and containers\n",
        "    variant_counts = {}\n",
        "    target_accessions = {}\n",
        "    metadata_entries = {}\n",
        "\n",
        "    logging.info(\"Processing metadata in a single pass...\")\n",
        "\n",
        "    # Single pass through metadata\n",
        "    for entry in tqdm.tqdm(read_jsonl(metadata_file), desc=\"Processing metadata\"):\n",
        "        if entry.get('completeness') != 'COMPLETE' or entry.get('isAnnotated') != True:\n",
        "            continue\n",
        "\n",
        "        variant = entry.get('virus', {}).get('pangolinClassification', 'Unknown')\n",
        "        accession = entry.get('accession', '').split('.')[0]\n",
        "\n",
        "        # Count variants (for first-pass equivalent)\n",
        "        variant_counts[variant] = variant_counts.get(variant, 0) + 1\n",
        "\n",
        "        # Store metadata entry for later use\n",
        "        metadata_entries[accession] = {\n",
        "            'variant': variant,\n",
        "            'species': entry.get('virus', {}).get('organismName', 'Unknown'),\n",
        "            'accession': accession\n",
        "        }\n",
        "\n",
        "    # Determine top variants\n",
        "    top_variants = sorted(variant_counts.items(), key=lambda x: x[1], reverse=True)[:top_n]\n",
        "    top_variant_names = [v[0] for v in top_variants]\n",
        "    logging.info(f\"Top {top_n} variants: {top_variant_names}\")\n",
        "\n",
        "    # Initialize containers for selected samples\n",
        "    selected_accessions = []\n",
        "    samples_collected = {v: 0 for v in top_variant_names}\n",
        "\n",
        "    # Select samples for each top variant\n",
        "    for accession, entry in metadata_entries.items():\n",
        "        variant = entry['variant']\n",
        "        if variant in top_variant_names and samples_collected[variant] < samples_per_variant:\n",
        "            selected_accessions.append(accession)\n",
        "            samples_collected[variant] += 1\n",
        "\n",
        "            # If we have enough samples for all variants, stop\n",
        "            if all(count >= samples_per_variant for count in samples_collected.values()):\n",
        "                break\n",
        "\n",
        "    logging.info(f\"Collected {len(selected_accessions)} accessions for processing\")\n",
        "\n",
        "    # Extract sequences (unchanged)\n",
        "    sequences = {}\n",
        "    for record in tqdm.tqdm(SeqIO.parse(cds_file, \"fasta\"), desc=\"Extracting sequences\"):\n",
        "        accession = record.id.split('.')[0]\n",
        "        if accession in selected_accessions:\n",
        "            sequences[accession] = str(record.seq)\n",
        "\n",
        "            # If we have all sequences, stop\n",
        "            if len(sequences) == len(selected_accessions):\n",
        "                break\n",
        "\n",
        "    # Create metadata list from stored entries\n",
        "    metadata = [\n",
        "        {\n",
        "            'accession': acc,\n",
        "            'variant': metadata_entries[acc]['variant'],\n",
        "            'species': metadata_entries[acc]['species'],\n",
        "            'sequence_length': len(sequences[acc])\n",
        "        }\n",
        "        for acc in selected_accessions if acc in sequences\n",
        "    ]\n",
        "\n",
        "    # Create DataFrame and AnnData with string indices\n",
        "    metadata_df = pd.DataFrame(metadata)\n",
        "    metadata_df.index = metadata_df['accession'].astype(str)\n",
        "\n",
        "    X = sp.csr_matrix((len(metadata_df), 1), dtype=np.float32)\n",
        "    var_names = ['spike_protein']\n",
        "\n",
        "    adata = ad.AnnData(X, obs=metadata_df, var=pd.DataFrame(index=var_names))\n",
        "    adata.layers['sequence'] = np.array([sequences[acc] for acc in metadata_df['accession']]).reshape(-1, 1)\n",
        "\n",
        "    # Encode categorical variables\n",
        "    adata.obs['variant'] = adata.obs['variant'].astype('category')\n",
        "    adata.obs['species'] = adata.obs['species'].astype('category')\n",
        "\n",
        "    logging.info(f\"Created AnnData object with {adata.n_obs} observations\")\n",
        "\n",
        "    return adata\n",
        "\n",
        "\n",
        "def prepare_for_model(adata, max_seq_length=None, chunk_size=10000):\n",
        "    \"\"\"\n",
        "    Prepare AnnData for model training with sequence truncation/padding.\n",
        "    \"\"\"\n",
        "\n",
        "    logging.info(f\"Preparing {adata.n_obs} sequences with chunk size {chunk_size}\")\n",
        "\n",
        "    processed_sequences = []\n",
        "\n",
        "    # Process sequences in chunks\n",
        "    for i in range(0, adata.n_obs, chunk_size):\n",
        "\n",
        "        # Get current chunk\n",
        "        end_idx = min(i + chunk_size, adata.n_obs)\n",
        "\n",
        "        # Extract sequence chunk\n",
        "        sequence_chunk = adata.layers['sequence'][i:end_idx]\n",
        "\n",
        "        # Process chunk\n",
        "        chunk_sequences = [seq[0] for seq in sequence_chunk]  # Extract sequence from shape (n, 1)\n",
        "\n",
        "        # Apply padding/truncation\n",
        "        if max_seq_length:\n",
        "            processed_chunk = [\n",
        "                seq[:max_seq_length] if len(seq) > max_seq_length else seq.ljust(max_seq_length, 'N')\n",
        "                for seq in chunk_sequences\n",
        "            ]\n",
        "        else:\n",
        "            processed_chunk = chunk_sequences\n",
        "\n",
        "        # Append processed sequences\n",
        "        processed_sequences.extend(processed_chunk)\n",
        "\n",
        "        # Force garbage collection\n",
        "        sequence_chunk = None\n",
        "        chunk_sequences = None\n",
        "        gc.collect()\n",
        "\n",
        "    # Store all processed sequences\n",
        "    logging.info(f\"Storing {len(processed_sequences)} processed sequences\")\n",
        "    adata.uns['processed_sequences'] = processed_sequences\n",
        "\n",
        "    return adata\n",
        "\n",
        "class SARSCoV2VAEDataset(Dataset):\n",
        "    def __init__(self, sequences, variants=None, max_length=None, one_hot=True, pad_to_power_of_two=True, augment=False, k=3):\n",
        "        self.variants = variants\n",
        "        self.one_hot = one_hot\n",
        "        self.augment = augment\n",
        "        self.k = k\n",
        "\n",
        "        # Determine padded length\n",
        "        self.max_length = max_length or max(len(seq) for seq in sequences)\n",
        "        if pad_to_power_of_two:\n",
        "            power = 1\n",
        "            while power < self.max_length:\n",
        "                power *= 2\n",
        "            self.max_length = power\n",
        "            logging.info(f\"Padded sequence length to {self.max_length} (power of 2)\")\n",
        "\n",
        "        self.sequences = sequences\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.sequences)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sequence = self.sequences[idx]\n",
        "        sequence = sequence[:self.max_length].ljust(self.max_length, 'N')\n",
        "\n",
        "        if self.augment and np.random.rand() > 0.5:\n",
        "            sequence = self.kmer_shuffle(sequence, self.k)\n",
        "\n",
        "        one_hot = self.one_hot_encode_torch(sequence)\n",
        "\n",
        "        item = {'input': one_hot, 'target': one_hot, 'index': idx}\n",
        "        if self.variants is not None:\n",
        "            item['variant'] = self.variants[idx]\n",
        "        return item\n",
        "\n",
        "    def one_hot_encode_torch(self, sequence):\n",
        "        map_dict = {'A': 0, 'T': 1, 'G': 2, 'C': 3}\n",
        "        seq_indices = torch.tensor([map_dict.get(n, 3) for n in sequence])\n",
        "        one_hot = torch.zeros(4, self.max_length)\n",
        "        one_hot.scatter_(0, seq_indices.unsqueeze(0), 1)\n",
        "        return one_hot\n",
        "\n",
        "    def kmer_shuffle(self, sequence, k=3):\n",
        "        kmers = [sequence[i:i+k] for i in range(len(sequence) - k + 1)]\n",
        "        np.random.shuffle(kmers)\n",
        "        return ''.join(kmers).ljust(self.max_length, 'N')[:self.max_length]\n",
        "\n",
        "\n",
        "def prepare_vae_data(adata, max_length=1024, batch_size=32, one_hot=True, train_test_ratio=0.8, augment=True):\n",
        "    train_idx, test_idx = train_test_split(\n",
        "        np.arange(adata.n_obs),\n",
        "        test_size=1 - train_test_ratio,\n",
        "        stratify=adata.obs['variant_encoded'] if 'variant_encoded' in adata.obs.columns else None,\n",
        "        random_state=42\n",
        "    )\n",
        "\n",
        "    train_dataset = SARSCoV2VAEDataset(\n",
        "        [adata.uns['processed_sequences'][i] for i in train_idx],\n",
        "        variants=adata.obs['variant_encoded'].values[train_idx] if 'variant_encoded' in adata.obs.columns else None,\n",
        "        max_length=max_length,\n",
        "        one_hot=one_hot,\n",
        "        augment=augment\n",
        "    )\n",
        "\n",
        "    test_dataset = SARSCoV2VAEDataset(\n",
        "        [adata.uns['processed_sequences'][i] for i in test_idx],\n",
        "        variants=adata.obs['variant_encoded'].values[test_idx] if 'variant_encoded' in adata.obs.columns else None,\n",
        "        max_length=max_length,\n",
        "        one_hot=one_hot,\n",
        "        augment=False\n",
        "    )\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=os.cpu_count(), pin_memory=True)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=os.cpu_count(), pin_memory=True)\n",
        "\n",
        "    return train_loader, test_loader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lKsD21NivtOV"
      },
      "source": [
        "## Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "sKEhZ2v5vY80"
      },
      "outputs": [],
      "source": [
        "class AttentionLayer(nn.Module):\n",
        "    def __init__(self, input_dim):\n",
        "        super().__init__()\n",
        "        self.attention = nn.Sequential(\n",
        "            nn.Linear(input_dim, input_dim),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(input_dim, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        weights = F.softmax(self.attention(x), dim=1)\n",
        "        return (weights * x).sum(dim=1)\n",
        "\n",
        "class ResidualBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, dropout_prob=0.3):\n",
        "        super().__init__()\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv1d(in_channels, out_channels, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm1d(out_channels),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Dropout(dropout_prob),\n",
        "            nn.Conv1d(out_channels, out_channels, kernel_size=3, padding=1)\n",
        "        )\n",
        "        self.shortcut = nn.Conv1d(in_channels, out_channels, kernel_size=1) if in_channels != out_channels else nn.Identity()\n",
        "\n",
        "    def forward(self, x):\n",
        "        return F.leaky_relu(self.conv(x) + self.shortcut(x), 0.2)\n",
        "\n",
        "class KLAnnealer:\n",
        "    def __init__(self, total_epochs, anneal_type='logistic'):\n",
        "        self.epoch = 0\n",
        "        self.total = total_epochs\n",
        "        self.anneal_type = anneal_type\n",
        "\n",
        "    def __call__(self):\n",
        "        if self.anneal_type == 'linear':\n",
        "            return min(1.0, self.epoch/self.total*4)\n",
        "        elif self.anneal_type == 'logistic':\n",
        "            return 1/(1 + np.exp(-0.05*(self.epoch - self.total//2)))\n",
        "        return 1.0\n",
        "\n",
        "class GenomicVAE(nn.Module):\n",
        "    def __init__(self, sequence_length, num_nucleotides=4, latent_dim=256, hidden_dims=[512, 256], dropout_prob=0.1):\n",
        "        super().__init__()\n",
        "        self.sequence_length = sequence_length\n",
        "        self.num_nucleotides = num_nucleotides\n",
        "        self.latent_dim = latent_dim\n",
        "\n",
        "        self.flat_dim = hidden_dims[-1] * sequence_length\n",
        "\n",
        "        # Encoder with Dropout\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.utils.spectral_norm(nn.Conv1d(num_nucleotides, hidden_dims[0], kernel_size=3, padding=1)),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Dropout(dropout_prob),\n",
        "            ResidualBlock(hidden_dims[0], hidden_dims[1], dropout_prob)\n",
        "        )\n",
        "\n",
        "        self.flatten = nn.Flatten()\n",
        "\n",
        "        # Attention mechanism for encoder\n",
        "        self.attention = AttentionLayer(hidden_dims[-1])\n",
        "\n",
        "        # Latent space\n",
        "        self.fc_mu = nn.Linear(self.flat_dim, latent_dim)\n",
        "        self.fc_var = nn.Linear(self.flat_dim, latent_dim)\n",
        "\n",
        "        # Decoder\n",
        "        self.decoder_fc = nn.Linear(latent_dim, self.flat_dim)\n",
        "\n",
        "        # Decoder conv layers\n",
        "        self.decoder_conv = nn.Sequential(\n",
        "            ResidualBlock(hidden_dims[-1], hidden_dims[-1], dropout_prob),\n",
        "            ResidualBlock(hidden_dims[-1], hidden_dims[0], dropout_prob),\n",
        "            nn.Conv1d(hidden_dims[0], num_nucleotides, kernel_size=3, padding=1)\n",
        "        )\n",
        "\n",
        "        self.kl_annealer = KLAnnealer(100, 'logistic')\n",
        "\n",
        "    def encode(self, x):\n",
        "        h = self.encoder(x)\n",
        "        h_flat = self.flatten(h)\n",
        "        return self.fc_mu(h_flat), self.fc_var(h_flat)\n",
        "\n",
        "    def decode(self, z):\n",
        "        h = self.decoder_fc(z)\n",
        "        h = h.view(-1, hidden_dims[-1], self.sequence_length)\n",
        "        h = F.dropout(h, p=0.1, training=self.training)\n",
        "        return self.decoder_conv(h)\n",
        "\n",
        "    def reparameterize(self, mu, log_var):\n",
        "        std = torch.exp(0.5 * log_var)\n",
        "        eps = torch.randn_like(std)\n",
        "        return mu + eps * std\n",
        "\n",
        "    def forward(self, x):\n",
        "        mu, log_var = self.encode(x)\n",
        "        z = self.reparameterize(mu, log_var)\n",
        "        return self.decode(z), mu, log_var\n",
        "\n",
        "    def loss_function(self, recon_x, x, mu, log_var, beta=0.5):\n",
        "        recon_loss = F.binary_cross_entropy_with_logits(recon_x, x, reduction='sum')\n",
        "        kl_div = -0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp())\n",
        "        return recon_loss + self.kl_annealer() * beta * kl_div\n",
        "\n",
        "def train_vae(model, train_loader, test_loader, epochs=50, lr=1e-3, beta=1.0, device='cuda', use_mixed_precision=True):\n",
        "    \"\"\"Train the model with proper mixed precision handling\"\"\"\n",
        "\n",
        "    device = torch.device(device if torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(device)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.5, patience=5)\n",
        "\n",
        "    scaler = GradScaler(enabled=use_mixed_precision)\n",
        "\n",
        "    # logs directory\n",
        "    log_dir = os.path.join(output_dir, 'logs')\n",
        "    os.makedirs(log_dir, exist_ok=True)\n",
        "    monitor = TrainingMonitor(log_dir)\n",
        "\n",
        "    train_losses = []\n",
        "    test_losses = []\n",
        "\n",
        "    for epoch in tqdm.trange(epochs, desc=\"Training Progress\"):\n",
        "        # Training phase\n",
        "        model.train()\n",
        "        train_loss = 0.0\n",
        "        current_lr = scheduler.optimizer.param_groups[0]['lr']\n",
        "        logging.info(f\"Current LR: {current_lr}\")\n",
        "\n",
        "        for batch in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            inputs = batch['input'].to(device)\n",
        "\n",
        "            with autocast(device_type='cuda' if device == 'cuda' else 'cpu', enabled=use_mixed_precision):\n",
        "                recon, mu, logvar = model(inputs)\n",
        "                loss = model.loss_function(recon, inputs, mu, logvar, beta=beta)\n",
        "\n",
        "            scaler.scale(loss).backward()\n",
        "            scaler.unscale_(optimizer)\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "\n",
        "\n",
        "            train_loss += loss.item()\n",
        "            model.kl_annealer.epoch += 1\n",
        "\n",
        "        # Validation phase\n",
        "        model.eval()\n",
        "        test_loss = 0.0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for batch in test_loader:\n",
        "                inputs = batch['input'].to(device)\n",
        "\n",
        "                with autocast(device_type='cuda' if device == 'cuda' else 'cpu', enabled=use_mixed_precision):\n",
        "                    recon, mu, logvar = model(inputs)\n",
        "                    loss = model.loss_function(recon, inputs, mu, logvar, beta=beta)\n",
        "\n",
        "                test_loss += loss.item()\n",
        "\n",
        "        # Log metrics\n",
        "        avg_train_loss = train_loss / len(train_loader)\n",
        "        avg_test_loss = test_loss / len(test_loader)\n",
        "        monitor.log('Loss/Train', avg_train_loss, epoch)\n",
        "        monitor.log('Loss/Test', avg_test_loss, epoch)\n",
        "\n",
        "        scheduler.step(avg_test_loss)\n",
        "\n",
        "        train_losses.append(avg_train_loss)\n",
        "        test_losses.append(avg_test_loss)\n",
        "\n",
        "        # Print progress\n",
        "        tqdm.tqdm.write(f\"Epoch {epoch+1}/{epochs} - Train Loss: {avg_train_loss:.4f}, Test Loss: {avg_test_loss:.4f}\")\n",
        "\n",
        "        # Memory cleanup\n",
        "        if torch.cuda.is_available():\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "    return model, train_losses, test_losses\n",
        "\n",
        "class TrainingMonitor:\n",
        "    def __init__(self, log_dir):\n",
        "        self.writer = SummaryWriter(log_dir)\n",
        "        self.metrics = dict()\n",
        "\n",
        "    def log(self, name, value, epoch):\n",
        "        self.writer.add_scalar(name, value, epoch)\n",
        "        if name not in self.metrics:\n",
        "            self.metrics[name] = []\n",
        "        self.metrics[name].append(value)\n",
        "\n",
        "    def plot_metrics(self):\n",
        "        plt.figure(figsize=(12, 6))\n",
        "        for name, values in self.metrics.items():\n",
        "            if len(values) > 0:\n",
        "                plt.plot(values, label=name)\n",
        "        if any(len(v) > 0 for v in self.metrics.values()):\n",
        "            plt.legend()\n",
        "        plt.grid(True, linestyle='--', alpha=0.7)\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.ylabel('Value')\n",
        "        plt.title('Training Metrics')\n",
        "        plt.savefig(os.path.join(output_dir, 'training_metrics.png'))\n",
        "        plt.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dp6S2emgvrTx"
      },
      "source": [
        "## Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "oxjxSu_DvX_u"
      },
      "outputs": [],
      "source": [
        "def extract_latent_vectors(model, data_loader, adata=None, device='cuda'):\n",
        "    \"\"\"Extract latent space representations from the trained VAE.\"\"\"\n",
        "    device = torch.device(device if torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    latent_vectors = []\n",
        "    dataset_indices = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, batch in enumerate(tqdm.tqdm(data_loader, desc=\"Extracting latent vectors\")):\n",
        "            inputs = batch['input'].to(device)\n",
        "            mu, _ = model.encode(inputs)\n",
        "            latent_vectors.append(mu.cpu().numpy())\n",
        "\n",
        "            batch_indices = batch['index'].cpu().numpy() if 'index' in batch else range(\n",
        "                batch_idx * data_loader.batch_size,\n",
        "                min((batch_idx + 1) * data_loader.batch_size, len(data_loader.dataset))\n",
        "            )\n",
        "            dataset_indices.extend(batch_indices)\n",
        "\n",
        "    latent_vectors = np.vstack(latent_vectors)\n",
        "\n",
        "    if adata is not None:\n",
        "        metadata_df = adata.obs.iloc[dataset_indices].copy()\n",
        "        return latent_vectors, metadata_df\n",
        "    else:\n",
        "        return latent_vectors, None\n",
        "\n",
        "def perform_clustering_and_visualization(latent_vectors, metadata, output_dir, model_name, n_neighbors=30, metric='correlation'):\n",
        "    # Create AnnData object\n",
        "    adata = sc.AnnData(latent_vectors)\n",
        "    adata.obs = metadata\n",
        "\n",
        "    # Compute neighborhood graph with correlation metric\n",
        "    sc.pp.neighbors(adata, n_neighbors=n_neighbors, use_rep='X', metric=metric, method='umap', knn=True)\n",
        "\n",
        "    # Run Leiden\n",
        "    resolutions = np.linspace(0.1, 2.0, 20)\n",
        "    ari_scores = []\n",
        "\n",
        "    for res in resolutions:\n",
        "        sc.tl.leiden(\n",
        "            adata,\n",
        "            resolution=res,\n",
        "            key_added=f'leiden_{res:.2f}',\n",
        "            flavor='igraph',\n",
        "            n_iterations=2,\n",
        "            directed=False\n",
        "        )\n",
        "        ari = adjusted_rand_score(adata.obs['variant'], adata.obs[f'leiden_{res:.2f}'])\n",
        "        ari_scores.append(ari)\n",
        "\n",
        "    best_res = resolutions[np.argmax(ari_scores)]\n",
        "    best_ari = max(ari_scores)\n",
        "    adata.obs['leiden'] = adata.obs[f'leiden_{best_res:.2f}']\n",
        "\n",
        "    sc.tl.umap(adata)\n",
        "\n",
        "    fig, axs = plt.subplots(1, 2, figsize=(20, 10))\n",
        "    sc.pl.umap(adata, color='variant', ax=axs[0], show=False)\n",
        "    sc.pl.umap(adata, color='leiden', ax=axs[1], show=False)\n",
        "    plt.suptitle(f\"{model_name} - UMAP (ARI: {best_ari:.4f})\")\n",
        "    plt.savefig(os.path.join(output_dir, f'{model_name.lower()}_umap.png'))\n",
        "    plt.close()\n",
        "\n",
        "    return best_ari\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7HijhZDZvzbC"
      },
      "source": [
        "## Main"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# cler gpu and cpu\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dcgSw9_bMbNS",
        "outputId": "67044c95-fc2a-4a99-9195-e54428ed8f07"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "54"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "jjlsCvNkFJPP"
      },
      "outputs": [],
      "source": [
        "# Setup logging\n",
        "for handler in logging.root.handlers[:]:\n",
        "    logging.root.removeHandler(handler)\n",
        "\n",
        "logging.basicConfig(\n",
        "    level=logging.DEBUG if \"--debug\" in sys.argv else logging.INFO,\n",
        "    format=\"%(asctime)s - %(levelname)s - %(message)s\",\n",
        "    handlers=[\n",
        "        logging.StreamHandler(sys.stdout),\n",
        "    ],\n",
        ")\n",
        "\n",
        "# GPU\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# DATA FROM NCBI\n",
        "data_dir = '/content/drive/MyDrive/540data/sarscov2'\n",
        "metadata_file = os.path.join(data_dir, 'spike_cds_metadata.tsv')\n",
        "cds_file = os.path.join(data_dir, 'cds_sequences/spike_cds.fna')\n",
        "data_report_jsonl = os.path.join(data_dir, 'spike_data/ncbi_dataset/data/data_report.jsonl')\n",
        "\n",
        "# EXPERIMENT\n",
        "output_dir = os.path.join(data_dir, 'results')\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# Set up training monitor\n",
        "log_dir = os.path.join(output_dir, 'tensorboard_logs')\n",
        "os.makedirs(log_dir, exist_ok=True)\n",
        "training_monitor = TrainingMonitor(log_dir)\n",
        "\n",
        "# Initialize data loaders\n",
        "train_loader_path = os.path.join(output_dir, 'train_loader.pt')\n",
        "test_loader_path = os.path.join(output_dir, 'test_loader.pt')\n",
        "\n",
        "# Model\n",
        "conv_model_path = os.path.join(output_dir, 'sars_cov2_conv_vae.pt')"
      ],
      "metadata": {
        "id": "QlKOY2eqOvQJ"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# EXPERIMENT PARAMETERS\n",
        "chunk_size = 1000  # For data processing\n",
        "top_n = 3 # Top variants by sample count\n",
        "samples_per_variant = 100  # Samples per variant - reduced from 10000 to be more realistic\n",
        "max_sequence_length = 2048 # power of 2\n",
        "num_nucleotides = 4 # A, C, G, T"
      ],
      "metadata": {
        "id": "RPsvXeNBR-H6"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# MODEL PARAMETERS\n",
        "latent_dim = 128\n",
        "hidden_dims = [256, 128]\n",
        "\n",
        "batch_size = 128\n",
        "epochs = 100\n",
        "lr = 1e-4\n",
        "beta = 2.0\n",
        "dropout_prob = .5\n",
        "\n",
        "use_mixed_precision = True\n",
        "\n",
        "# clustering viz\n",
        "n_neighbors=30\n",
        "metric='correlation'"
      ],
      "metadata": {
        "id": "BBsVG5fVLoPD"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Process data efficiently - directly get filtered data\n",
        "filtered_data_path = os.path.join(output_dir, 'filtered_data.h5ad')\n",
        "if not os.path.exists(filtered_data_path):\n",
        "    logging.info(\"Processing and filtering data...\")\n",
        "    adata_filtered = process_data(\n",
        "        cds_file,\n",
        "        data_report_jsonl,\n",
        "        top_n=top_n,\n",
        "        samples_per_variant=samples_per_variant,\n",
        "        chunk_size=chunk_size\n",
        "    )\n",
        "\n",
        "    if adata_filtered is None:\n",
        "        logging.error(\"Failed to process data. Exiting.\")\n",
        "        sys.exit(1)\n",
        "\n",
        "    # Prepare sequences for model\n",
        "    adata_filtered = prepare_for_model(adata_filtered, max_seq_length=max_sequence_length)\n",
        "    adata_filtered.write(filtered_data_path)\n",
        "else:\n",
        "    logging.info(\"Loading filtered data from file...\")\n",
        "    adata_filtered = sc.read(filtered_data_path)\n",
        "\n",
        "# Log dataset information\n",
        "logging.info(f\"Filtered dataset size: {adata_filtered.n_obs} sequences\")\n",
        "logging.info(f\"Samples per variant:\\n{adata_filtered.obs['variant'].value_counts()}\")\n",
        "logging.info(f\"Unique species: {adata_filtered.obs['species'].nunique()}\")\n",
        "logging.info(f\"Species distribution:\\n{adata_filtered.obs['species'].value_counts()}\")\n",
        "logging.info(f\"Unique variants: {adata_filtered.obs['variant'].nunique()}\")\n",
        "logging.info(f\"Variant distribution (top {top_n}):\\n{adata_filtered.obs['variant'].value_counts().head(top_n)}\")\n",
        "\n",
        "# Free up memory after data processing\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.empty_cache()\n",
        "gc.collect()\n",
        "\n",
        "if os.path.exists(train_loader_path) and os.path.exists(test_loader_path):\n",
        "    train_loader = torch.load(train_loader_path, weights_only=False)\n",
        "    test_loader = torch.load(test_loader_path, weights_only=False)\n",
        "    logging.info(\"Loaded existing data loaders\")\n",
        "else:\n",
        "    train_loader, test_loader = prepare_vae_data(\n",
        "        adata_filtered,\n",
        "        max_length=max_sequence_length,\n",
        "        batch_size=batch_size\n",
        "    )\n",
        "    torch.save(train_loader, train_loader_path)\n",
        "    torch.save(test_loader, test_loader_path)\n",
        "    logging.info(\"Created and saved new data loaders\")\n",
        "\n",
        "logging.info(f\"Train loader size: {len(train_loader)} batches\")\n",
        "logging.info(f\"Test loader size: {len(test_loader)} batches\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OrJFnpXmiY6U",
        "outputId": "e39ff3f2-9b05-4a68-9bb4-be24dc85b3a5"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-03-26 23:06:37,708 - INFO - Processing and filtering data...\n",
            "2025-03-26 23:06:37,710 - INFO - Processing metadata in a single pass...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing metadata: 9088286it [06:57, 21743.51it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-03-26 23:13:35,691 - INFO - Top 3 variants: ['BA.1.1', 'B.1.1.7', 'BA.2.12.1']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-03-26 23:13:35,874 - INFO - Collected 300 accessions for processing\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Extracting sequences: 2779175it [01:33, 29864.75it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-03-26 23:15:09,181 - INFO - Created AnnData object with 300 observations\n",
            "2025-03-26 23:15:09,412 - INFO - Preparing 300 sequences with chunk size 10000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-03-26 23:15:09,909 - INFO - Storing 300 processed sequences\n",
            "2025-03-26 23:15:10,004 - INFO - Filtered dataset size: 300 sequences\n",
            "2025-03-26 23:15:10,013 - INFO - Samples per variant:\n",
            "variant\n",
            "B.1.1.7      100\n",
            "BA.1.1       100\n",
            "BA.2.12.1    100\n",
            "Name: count, dtype: int64\n",
            "2025-03-26 23:15:10,017 - INFO - Unique species: 1\n",
            "2025-03-26 23:15:10,018 - INFO - Species distribution:\n",
            "species\n",
            "Severe acute respiratory syndrome coronavirus 2    300\n",
            "Name: count, dtype: int64\n",
            "2025-03-26 23:15:10,020 - INFO - Unique variants: 3\n",
            "2025-03-26 23:15:10,023 - INFO - Variant distribution (top 3):\n",
            "variant\n",
            "B.1.1.7      100\n",
            "BA.1.1       100\n",
            "BA.2.12.1    100\n",
            "Name: count, dtype: int64\n",
            "2025-03-26 23:15:10,578 - INFO - Padded sequence length to 2048 (power of 2)\n",
            "2025-03-26 23:15:10,580 - INFO - Padded sequence length to 2048 (power of 2)\n",
            "2025-03-26 23:15:10,605 - INFO - Created and saved new data loaders\n",
            "2025-03-26 23:15:10,607 - INFO - Train loader size: 2 batches\n",
            "2025-03-26 23:15:10,610 - INFO - Test loader size: 1 batches\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''Initialize Model'''\n",
        "# Initialize conventional model\n",
        "conv_model = GenomicVAE(\n",
        "    sequence_length=max_sequence_length,\n",
        "    num_nucleotides=num_nucleotides,\n",
        "    latent_dim=latent_dim,\n",
        "    hidden_dims=hidden_dims,\n",
        "    dropout_prob=dropout_prob\n",
        ").to(device)\n",
        "\n",
        "# Free up memory\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.empty_cache()\n",
        "gc.collect()\n",
        "\n",
        "'''Train Model'''\n",
        "# Train model with augmentation\n",
        "if not os.path.exists(conv_model_path):\n",
        "    logging.info(f\"Training Conventional VAE with augmentation\")\n",
        "\n",
        "    # Create training monitor\n",
        "    monitor = TrainingMonitor(os.path.join(log_dir, 'conv_vae'))\n",
        "\n",
        "    # Train with augmentation\n",
        "    conv_model, conv_train_losses, conv_test_losses = train_vae(\n",
        "        conv_model,\n",
        "        train_loader,\n",
        "        test_loader,\n",
        "        epochs=epochs,\n",
        "        beta=beta,\n",
        "        device=device,\n",
        "        lr=lr,\n",
        "        use_mixed_precision=use_mixed_precision\n",
        "    )\n",
        "\n",
        "    # Save the trained model\n",
        "    torch.save(conv_model.state_dict(), conv_model_path)\n",
        "\n",
        "    # Plot training curves\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    plt.plot(conv_train_losses, label='Train Loss')\n",
        "    plt.plot(conv_test_losses, label='Test Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.title('VAE Training Progress')\n",
        "    plt.legend()\n",
        "    plt.savefig(os.path.join(output_dir, 'training_curves.png'))\n",
        "    plt.close()\n",
        "\n",
        "    # Create Tensorboard training visualization\n",
        "    monitor.plot_metrics()\n",
        "else:\n",
        "    logging.info(f\"Loading trained conventional model from {conv_model_path}\")\n",
        "    conv_model.load_state_dict(torch.load(conv_model_path, map_location=device))\n",
        "    conv_model.to(device)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PYJGmoBtzkJB",
        "outputId": "cbd85a3b-1127-40bd-b94c-de12dc45e08e"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-03-26 23:15:24,320 - INFO - Training Conventional VAE with augmentation\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining Progress:   0%|          | 0/100 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-03-26 23:15:24,361 - INFO - Current LR: 0.0001\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:   1%|          | 1/100 [00:01<02:36,  1.58s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100 - Train Loss: 666950.3750, Test Loss: 328862.9688\n",
            "2025-03-26 23:15:25,942 - INFO - Current LR: 0.0001\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:   2%|▏         | 2/100 [00:03<02:31,  1.55s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2/100 - Train Loss: 627089.5625, Test Loss: 322755.7812\n",
            "2025-03-26 23:15:27,466 - INFO - Current LR: 0.0001\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:   3%|▎         | 3/100 [00:04<02:42,  1.68s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3/100 - Train Loss: 593568.8750, Test Loss: 310034.1250\n",
            "2025-03-26 23:15:29,296 - INFO - Current LR: 0.0001\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:   4%|▍         | 4/100 [00:06<02:45,  1.72s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4/100 - Train Loss: 565535.1562, Test Loss: 302161.0938\n",
            "2025-03-26 23:15:31,089 - INFO - Current LR: 0.0001\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:   5%|▌         | 5/100 [00:08<02:38,  1.67s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5/100 - Train Loss: 547441.4375, Test Loss: 298745.0625\n",
            "2025-03-26 23:15:32,663 - INFO - Current LR: 0.0001\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:   6%|▌         | 6/100 [00:10<02:43,  1.74s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6/100 - Train Loss: 534252.5625, Test Loss: 290988.4062\n",
            "2025-03-26 23:15:34,546 - INFO - Current LR: 0.0001\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:   7%|▋         | 7/100 [00:12<02:55,  1.89s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 7/100 - Train Loss: 527214.6406, Test Loss: 284808.5938\n",
            "2025-03-26 23:15:36,744 - INFO - Current LR: 0.0001\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:   8%|▊         | 8/100 [00:13<02:43,  1.77s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 8/100 - Train Loss: 525373.2031, Test Loss: 281194.8750\n",
            "2025-03-26 23:15:38,267 - INFO - Current LR: 0.0001\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:   9%|▉         | 9/100 [00:15<02:34,  1.69s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 9/100 - Train Loss: 512674.3438, Test Loss: 275804.4375\n",
            "2025-03-26 23:15:39,781 - INFO - Current LR: 0.0001\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  10%|█         | 10/100 [00:17<02:32,  1.69s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10/100 - Train Loss: 505088.4688, Test Loss: 267089.1562\n",
            "2025-03-26 23:15:41,469 - INFO - Current LR: 0.0001\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  11%|█         | 11/100 [00:19<02:36,  1.76s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 11/100 - Train Loss: 490216.1875, Test Loss: 258337.6406\n",
            "2025-03-26 23:15:43,382 - INFO - Current LR: 0.0001\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  12%|█▏        | 12/100 [00:20<02:32,  1.73s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 12/100 - Train Loss: 478967.1562, Test Loss: 248226.1406\n",
            "2025-03-26 23:15:45,050 - INFO - Current LR: 0.0001\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  13%|█▎        | 13/100 [00:22<02:25,  1.67s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 13/100 - Train Loss: 465405.0156, Test Loss: 237350.6406\n",
            "2025-03-26 23:15:46,582 - INFO - Current LR: 0.0001\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  14%|█▍        | 14/100 [00:23<02:19,  1.63s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 14/100 - Train Loss: 452016.3594, Test Loss: 225147.1562\n",
            "2025-03-26 23:15:48,107 - INFO - Current LR: 0.0001\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  15%|█▌        | 15/100 [00:25<02:15,  1.59s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 15/100 - Train Loss: 425335.7031, Test Loss: 212618.3750\n",
            "2025-03-26 23:15:49,621 - INFO - Current LR: 0.0001\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  16%|█▌        | 16/100 [00:26<02:12,  1.58s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 16/100 - Train Loss: 435681.2812, Test Loss: 201251.4531\n",
            "2025-03-26 23:15:51,175 - INFO - Current LR: 0.0001\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  17%|█▋        | 17/100 [00:28<02:09,  1.56s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 17/100 - Train Loss: 415537.7656, Test Loss: 192810.4688\n",
            "2025-03-26 23:15:52,699 - INFO - Current LR: 0.0001\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  18%|█▊        | 18/100 [00:29<02:07,  1.55s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 18/100 - Train Loss: 424499.5000, Test Loss: 182428.5781\n",
            "2025-03-26 23:15:54,218 - INFO - Current LR: 0.0001\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  19%|█▉        | 19/100 [00:31<02:13,  1.65s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 19/100 - Train Loss: 396755.9688, Test Loss: 169236.9375\n",
            "2025-03-26 23:15:56,101 - INFO - Current LR: 0.0001\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  20%|██        | 20/100 [00:33<02:14,  1.68s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 20/100 - Train Loss: 369424.2656, Test Loss: 153725.0625\n",
            "2025-03-26 23:15:57,854 - INFO - Current LR: 0.0001\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  21%|██        | 21/100 [00:35<02:09,  1.64s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 21/100 - Train Loss: 370099.9531, Test Loss: 144511.4844\n",
            "2025-03-26 23:15:59,414 - INFO - Current LR: 0.0001\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  22%|██▏       | 22/100 [00:36<02:05,  1.61s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 22/100 - Train Loss: 355301.3438, Test Loss: 132896.0781\n",
            "2025-03-26 23:16:00,944 - INFO - Current LR: 0.0001\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  23%|██▎       | 23/100 [00:38<02:02,  1.59s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 23/100 - Train Loss: 344333.5938, Test Loss: 126006.8906\n",
            "2025-03-26 23:16:02,497 - INFO - Current LR: 0.0001\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  24%|██▍       | 24/100 [00:39<02:01,  1.60s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 24/100 - Train Loss: 341850.8594, Test Loss: 117852.1562\n",
            "2025-03-26 23:16:04,110 - INFO - Current LR: 0.0001\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  25%|██▌       | 25/100 [00:41<01:58,  1.59s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 25/100 - Train Loss: 350067.5469, Test Loss: 106548.9844\n",
            "2025-03-26 23:16:05,664 - INFO - Current LR: 0.0001\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  26%|██▌       | 26/100 [00:42<01:56,  1.57s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 26/100 - Train Loss: 346841.3281, Test Loss: 95483.8672\n",
            "2025-03-26 23:16:07,199 - INFO - Current LR: 0.0001\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  27%|██▋       | 27/100 [00:44<01:57,  1.61s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 27/100 - Train Loss: 350743.7188, Test Loss: 92542.2969\n",
            "2025-03-26 23:16:08,895 - INFO - Current LR: 0.0001\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  28%|██▊       | 28/100 [00:46<02:02,  1.70s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 28/100 - Train Loss: 341188.5781, Test Loss: 78751.8750\n",
            "2025-03-26 23:16:10,804 - INFO - Current LR: 0.0001\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  29%|██▉       | 29/100 [00:47<01:56,  1.65s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 29/100 - Train Loss: 311270.0625, Test Loss: 77187.4844\n",
            "2025-03-26 23:16:12,327 - INFO - Current LR: 0.0001\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  30%|███       | 30/100 [00:49<01:53,  1.61s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 30/100 - Train Loss: 325057.6094, Test Loss: 67936.1719\n",
            "2025-03-26 23:16:13,869 - INFO - Current LR: 0.0001\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  31%|███       | 31/100 [00:51<01:49,  1.59s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 31/100 - Train Loss: 330025.2969, Test Loss: 65867.9297\n",
            "2025-03-26 23:16:15,397 - INFO - Current LR: 0.0001\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  32%|███▏      | 32/100 [00:52<01:47,  1.57s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 32/100 - Train Loss: 350835.2344, Test Loss: 50852.3516\n",
            "2025-03-26 23:16:16,940 - INFO - Current LR: 0.0001\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  33%|███▎      | 33/100 [00:54<01:44,  1.56s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 33/100 - Train Loss: 329409.9531, Test Loss: 51152.9102\n",
            "2025-03-26 23:16:18,474 - INFO - Current LR: 0.0001\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  34%|███▍      | 34/100 [00:55<01:42,  1.56s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 34/100 - Train Loss: 306454.7969, Test Loss: 50755.5117\n",
            "2025-03-26 23:16:20,022 - INFO - Current LR: 0.0001\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  35%|███▌      | 35/100 [00:57<01:45,  1.62s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 35/100 - Train Loss: 311897.4219, Test Loss: 44589.1133\n",
            "2025-03-26 23:16:21,799 - INFO - Current LR: 0.0001\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  36%|███▌      | 36/100 [00:59<01:48,  1.70s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 36/100 - Train Loss: 310917.6562, Test Loss: 51761.4258\n",
            "2025-03-26 23:16:23,684 - INFO - Current LR: 0.0001\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  37%|███▋      | 37/100 [01:01<01:47,  1.70s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 37/100 - Train Loss: 310280.3281, Test Loss: 52754.9570\n",
            "2025-03-26 23:16:25,380 - INFO - Current LR: 0.0001\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  38%|███▊      | 38/100 [01:02<01:42,  1.65s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 38/100 - Train Loss: 310967.2500, Test Loss: 46646.0781\n",
            "2025-03-26 23:16:26,897 - INFO - Current LR: 0.0001\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  39%|███▉      | 39/100 [01:04<01:38,  1.62s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 39/100 - Train Loss: 323374.9844, Test Loss: 45173.8438\n",
            "2025-03-26 23:16:28,442 - INFO - Current LR: 0.0001\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  40%|████      | 40/100 [01:05<01:35,  1.59s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 40/100 - Train Loss: 310652.2188, Test Loss: 47454.9180\n",
            "2025-03-26 23:16:29,967 - INFO - Current LR: 0.0001\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  41%|████      | 41/100 [01:07<01:33,  1.59s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 41/100 - Train Loss: 288822.5312, Test Loss: 47090.2500\n",
            "2025-03-26 23:16:31,550 - INFO - Current LR: 5e-05\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  42%|████▏     | 42/100 [01:08<01:31,  1.57s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 42/100 - Train Loss: 320716.5781, Test Loss: 48652.7266\n",
            "2025-03-26 23:16:33,088 - INFO - Current LR: 5e-05\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  43%|████▎     | 43/100 [01:10<01:32,  1.62s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 43/100 - Train Loss: 282208.3438, Test Loss: 47765.5547\n",
            "2025-03-26 23:16:34,821 - INFO - Current LR: 5e-05\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  44%|████▍     | 44/100 [01:12<01:36,  1.72s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 44/100 - Train Loss: 323019.6094, Test Loss: 49190.0469\n",
            "2025-03-26 23:16:36,787 - INFO - Current LR: 5e-05\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  45%|████▌     | 45/100 [01:14<01:34,  1.72s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 45/100 - Train Loss: 307025.9688, Test Loss: 50601.7734\n",
            "2025-03-26 23:16:38,480 - INFO - Current LR: 5e-05\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  46%|████▌     | 46/100 [01:15<01:29,  1.66s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 46/100 - Train Loss: 323937.9375, Test Loss: 42531.5391\n",
            "2025-03-26 23:16:40,020 - INFO - Current LR: 5e-05\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  47%|████▋     | 47/100 [01:17<01:31,  1.72s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 47/100 - Train Loss: 319324.8438, Test Loss: 40165.2734\n",
            "2025-03-26 23:16:41,867 - INFO - Current LR: 5e-05\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  48%|████▊     | 48/100 [01:19<01:30,  1.74s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 48/100 - Train Loss: 325115.5938, Test Loss: 44257.4648\n",
            "2025-03-26 23:16:43,667 - INFO - Current LR: 5e-05\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  49%|████▉     | 49/100 [01:20<01:25,  1.68s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 49/100 - Train Loss: 291972.0469, Test Loss: 53814.9648\n",
            "2025-03-26 23:16:45,190 - INFO - Current LR: 5e-05\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  50%|█████     | 50/100 [01:22<01:21,  1.63s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 50/100 - Train Loss: 290171.9062, Test Loss: 49412.0938\n",
            "2025-03-26 23:16:46,710 - INFO - Current LR: 5e-05\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  51%|█████     | 51/100 [01:24<01:21,  1.67s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 51/100 - Train Loss: 284170.2969, Test Loss: 43621.7500\n",
            "2025-03-26 23:16:48,459 - INFO - Current LR: 5e-05\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  52%|█████▏    | 52/100 [01:25<01:22,  1.73s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 52/100 - Train Loss: 320223.1562, Test Loss: 47418.1484\n",
            "2025-03-26 23:16:50,332 - INFO - Current LR: 5e-05\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  53%|█████▎    | 53/100 [01:27<01:20,  1.71s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 53/100 - Train Loss: 303594.2812, Test Loss: 54136.9805\n",
            "2025-03-26 23:16:52,004 - INFO - Current LR: 2.5e-05\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  54%|█████▍    | 54/100 [01:29<01:16,  1.66s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 54/100 - Train Loss: 294998.4219, Test Loss: 50862.8867\n",
            "2025-03-26 23:16:53,544 - INFO - Current LR: 2.5e-05\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  55%|█████▌    | 55/100 [01:30<01:12,  1.62s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 55/100 - Train Loss: 281516.1875, Test Loss: 48519.6719\n",
            "2025-03-26 23:16:55,070 - INFO - Current LR: 2.5e-05\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  56%|█████▌    | 56/100 [01:32<01:10,  1.60s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 56/100 - Train Loss: 308715.3594, Test Loss: 48811.2344\n",
            "2025-03-26 23:16:56,618 - INFO - Current LR: 2.5e-05\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  57%|█████▋    | 57/100 [01:33<01:07,  1.57s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 57/100 - Train Loss: 335542.2031, Test Loss: 44974.2266\n",
            "2025-03-26 23:16:58,132 - INFO - Current LR: 2.5e-05\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  58%|█████▊    | 58/100 [01:35<01:05,  1.56s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 58/100 - Train Loss: 295767.9375, Test Loss: 44692.5234\n",
            "2025-03-26 23:16:59,677 - INFO - Current LR: 2.5e-05\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  59%|█████▉    | 59/100 [01:36<01:05,  1.60s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 59/100 - Train Loss: 328523.8281, Test Loss: 49760.9297\n",
            "2025-03-26 23:17:01,353 - INFO - Current LR: 1.25e-05\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  60%|██████    | 60/100 [01:38<01:06,  1.67s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 60/100 - Train Loss: 318929.4688, Test Loss: 49152.6289\n",
            "2025-03-26 23:17:03,176 - INFO - Current LR: 1.25e-05\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  61%|██████    | 61/100 [01:40<01:05,  1.67s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 61/100 - Train Loss: 295361.2812, Test Loss: 48258.5039\n",
            "2025-03-26 23:17:04,870 - INFO - Current LR: 1.25e-05\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  62%|██████▏   | 62/100 [01:42<01:01,  1.63s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 62/100 - Train Loss: 297420.8438, Test Loss: 50729.6875\n",
            "2025-03-26 23:17:06,382 - INFO - Current LR: 1.25e-05\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  63%|██████▎   | 63/100 [01:43<00:59,  1.59s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 63/100 - Train Loss: 285154.3750, Test Loss: 50041.8828\n",
            "2025-03-26 23:17:07,904 - INFO - Current LR: 1.25e-05\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  64%|██████▍   | 64/100 [01:45<00:56,  1.58s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 64/100 - Train Loss: 294860.2266, Test Loss: 49693.0469\n",
            "2025-03-26 23:17:09,443 - INFO - Current LR: 1.25e-05\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  65%|██████▌   | 65/100 [01:46<00:54,  1.56s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 65/100 - Train Loss: 276592.5625, Test Loss: 49660.8125\n",
            "2025-03-26 23:17:10,967 - INFO - Current LR: 6.25e-06\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  66%|██████▌   | 66/100 [01:48<00:53,  1.56s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 66/100 - Train Loss: 278598.5781, Test Loss: 50672.2461\n",
            "2025-03-26 23:17:12,521 - INFO - Current LR: 6.25e-06\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  67%|██████▋   | 67/100 [01:49<00:51,  1.55s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 67/100 - Train Loss: 282149.4844, Test Loss: 54633.6055\n",
            "2025-03-26 23:17:14,043 - INFO - Current LR: 6.25e-06\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  68%|██████▊   | 68/100 [01:51<00:50,  1.59s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 68/100 - Train Loss: 312396.2812, Test Loss: 51955.6680\n",
            "2025-03-26 23:17:15,731 - INFO - Current LR: 6.25e-06\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  69%|██████▉   | 69/100 [01:53<00:52,  1.68s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 69/100 - Train Loss: 308209.4219, Test Loss: 54715.3047\n",
            "2025-03-26 23:17:17,621 - INFO - Current LR: 6.25e-06\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  70%|███████   | 70/100 [01:54<00:49,  1.64s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 70/100 - Train Loss: 309049.9688, Test Loss: 55254.9688\n",
            "2025-03-26 23:17:19,179 - INFO - Current LR: 6.25e-06\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  71%|███████   | 71/100 [01:56<00:46,  1.61s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 71/100 - Train Loss: 313564.6406, Test Loss: 54078.9141\n",
            "2025-03-26 23:17:20,712 - INFO - Current LR: 3.125e-06\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  72%|███████▏  | 72/100 [01:57<00:44,  1.59s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 72/100 - Train Loss: 301101.9688, Test Loss: 54009.6523\n",
            "2025-03-26 23:17:22,240 - INFO - Current LR: 3.125e-06\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  73%|███████▎  | 73/100 [01:59<00:42,  1.58s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 73/100 - Train Loss: 293205.2969, Test Loss: 52229.6172\n",
            "2025-03-26 23:17:23,811 - INFO - Current LR: 3.125e-06\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  74%|███████▍  | 74/100 [02:00<00:40,  1.57s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 74/100 - Train Loss: 277999.1562, Test Loss: 54262.2852\n",
            "2025-03-26 23:17:25,347 - INFO - Current LR: 3.125e-06\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  75%|███████▌  | 75/100 [02:02<00:39,  1.56s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 75/100 - Train Loss: 311264.7656, Test Loss: 52314.5547\n",
            "2025-03-26 23:17:26,892 - INFO - Current LR: 3.125e-06\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  76%|███████▌  | 76/100 [02:04<00:38,  1.62s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 76/100 - Train Loss: 304974.6875, Test Loss: 50653.0195\n",
            "2025-03-26 23:17:28,642 - INFO - Current LR: 3.125e-06\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  77%|███████▋  | 77/100 [02:06<00:38,  1.69s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 77/100 - Train Loss: 312952.0781, Test Loss: 51081.2188\n",
            "2025-03-26 23:17:30,494 - INFO - Current LR: 1.5625e-06\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  78%|███████▊  | 78/100 [02:07<00:37,  1.70s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 78/100 - Train Loss: 303392.9844, Test Loss: 50744.8008\n",
            "2025-03-26 23:17:32,216 - INFO - Current LR: 1.5625e-06\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  79%|███████▉  | 79/100 [02:09<00:34,  1.64s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 79/100 - Train Loss: 307278.2656, Test Loss: 52775.9219\n",
            "2025-03-26 23:17:33,734 - INFO - Current LR: 1.5625e-06\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  80%|████████  | 80/100 [02:10<00:32,  1.61s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 80/100 - Train Loss: 309828.3594, Test Loss: 54070.8125\n",
            "2025-03-26 23:17:35,268 - INFO - Current LR: 1.5625e-06\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  81%|████████  | 81/100 [02:12<00:30,  1.58s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 81/100 - Train Loss: 283504.9531, Test Loss: 51712.2734\n",
            "2025-03-26 23:17:36,789 - INFO - Current LR: 1.5625e-06\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  82%|████████▏ | 82/100 [02:13<00:28,  1.57s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 82/100 - Train Loss: 291071.1094, Test Loss: 53028.1562\n",
            "2025-03-26 23:17:38,326 - INFO - Current LR: 1.5625e-06\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  83%|████████▎ | 83/100 [02:15<00:26,  1.55s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 83/100 - Train Loss: 275484.2812, Test Loss: 51804.6016\n",
            "2025-03-26 23:17:39,837 - INFO - Current LR: 7.8125e-07\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  84%|████████▍ | 84/100 [02:17<00:25,  1.59s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 84/100 - Train Loss: 302440.7969, Test Loss: 52731.1406\n",
            "2025-03-26 23:17:41,531 - INFO - Current LR: 7.8125e-07\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  85%|████████▌ | 85/100 [02:19<00:25,  1.71s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 85/100 - Train Loss: 270115.7266, Test Loss: 54284.4141\n",
            "2025-03-26 23:17:43,497 - INFO - Current LR: 7.8125e-07\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  86%|████████▌ | 86/100 [02:20<00:24,  1.72s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 86/100 - Train Loss: 294801.5859, Test Loss: 52863.0664\n",
            "2025-03-26 23:17:45,236 - INFO - Current LR: 7.8125e-07\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  87%|████████▋ | 87/100 [02:22<00:21,  1.67s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 87/100 - Train Loss: 286157.9531, Test Loss: 54328.2461\n",
            "2025-03-26 23:17:46,792 - INFO - Current LR: 7.8125e-07\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  88%|████████▊ | 88/100 [02:23<00:19,  1.63s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 88/100 - Train Loss: 336591.3750, Test Loss: 51888.1914\n",
            "2025-03-26 23:17:48,338 - INFO - Current LR: 7.8125e-07\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  89%|████████▉ | 89/100 [02:25<00:17,  1.60s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 89/100 - Train Loss: 297359.4688, Test Loss: 52610.7383\n",
            "2025-03-26 23:17:49,866 - INFO - Current LR: 3.90625e-07\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  90%|█████████ | 90/100 [02:27<00:15,  1.57s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 90/100 - Train Loss: 314708.8594, Test Loss: 52950.0156\n",
            "2025-03-26 23:17:51,379 - INFO - Current LR: 3.90625e-07\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  91%|█████████ | 91/100 [02:28<00:14,  1.56s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 91/100 - Train Loss: 257477.0156, Test Loss: 50051.9609\n",
            "2025-03-26 23:17:52,907 - INFO - Current LR: 3.90625e-07\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  92%|█████████▏| 92/100 [02:30<00:12,  1.56s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 92/100 - Train Loss: 291971.3438, Test Loss: 53509.1602\n",
            "2025-03-26 23:17:54,464 - INFO - Current LR: 3.90625e-07\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  93%|█████████▎| 93/100 [02:31<00:11,  1.62s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 93/100 - Train Loss: 306182.2812, Test Loss: 55203.3086\n",
            "2025-03-26 23:17:56,233 - INFO - Current LR: 3.90625e-07\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  94%|█████████▍| 94/100 [02:33<00:10,  1.68s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 94/100 - Train Loss: 309121.2578, Test Loss: 52478.5352\n",
            "2025-03-26 23:17:58,056 - INFO - Current LR: 3.90625e-07\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  95%|█████████▌| 95/100 [02:35<00:08,  1.64s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 95/100 - Train Loss: 305576.8125, Test Loss: 52843.9648\n",
            "2025-03-26 23:17:59,585 - INFO - Current LR: 1.953125e-07\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  96%|█████████▌| 96/100 [02:36<00:06,  1.60s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 96/100 - Train Loss: 307250.9531, Test Loss: 52478.4922\n",
            "2025-03-26 23:18:01,101 - INFO - Current LR: 1.953125e-07\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  97%|█████████▋| 97/100 [02:38<00:04,  1.58s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 97/100 - Train Loss: 309721.1406, Test Loss: 51938.4688\n",
            "2025-03-26 23:18:02,630 - INFO - Current LR: 1.953125e-07\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  98%|█████████▊| 98/100 [02:39<00:03,  1.57s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 98/100 - Train Loss: 329342.7031, Test Loss: 51870.1641\n",
            "2025-03-26 23:18:04,185 - INFO - Current LR: 1.953125e-07\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  99%|█████████▉| 99/100 [02:41<00:01,  1.57s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 99/100 - Train Loss: 298237.8906, Test Loss: 51319.6641\n",
            "2025-03-26 23:18:05,744 - INFO - Current LR: 1.953125e-07\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress: 100%|██████████| 100/100 [02:42<00:00,  1.63s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 100/100 - Train Loss: 307444.9062, Test Loss: 49815.0273\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''Extract Latent Representations'''\n",
        "logging.info(\"Extracting latent space representations...\")\n",
        "conv_latent, conv_meta = extract_latent_vectors(conv_model, test_loader, adata=adata_filtered, device=device)\n",
        "logging.info(f\"Extracted {len(conv_latent)} latent vectors\")\n",
        "\n",
        "'''Perform Clustering and Visualization'''\n",
        "logging.info(\"Performing clustering and visualization...\")\n",
        "conv_ari = perform_clustering_and_visualization(\n",
        "    latent_vectors=conv_latent,\n",
        "    metadata=conv_meta,\n",
        "    output_dir=output_dir,\n",
        "    model_name=\"Conventional\",\n",
        "    n_neighbors=n_neighbors,\n",
        "    metric=metric\n",
        ")\n",
        "\n",
        "logging.info(f\"\\nClustering Results:\")\n",
        "logging.info(f\"Conventional Model ARI: {conv_ari:.4f}\")\n",
        "\n",
        "# Clean up at the end\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.empty_cache()\n",
        "gc.collect()\n",
        "\n",
        "logging.info(\"Analysis complete!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Yk0LT-_LhCj",
        "outputId": "ccadf0eb-d712-4135-d0e0-8ab4ccf20805"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-03-26 23:18:16,677 - INFO - Extracting latent space representations...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Extracting latent vectors: 100%|██████████| 1/1 [00:00<00:00,  4.05it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-03-26 23:18:16,932 - INFO - Extracted 60 latent vectors\n",
            "2025-03-26 23:18:16,933 - INFO - Performing clustering and visualization...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-03-26 23:18:17,582 - INFO - \n",
            "Clustering Results:\n",
            "2025-03-26 23:18:17,583 - INFO - Conventional Model ARI: 0.0000\n",
            "2025-03-26 23:18:18,095 - INFO - Analysis complete!\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "kdeo6piwLOOb",
        "PSlosa6ZE7tx",
        "Na4-h8NtLxsM",
        "lKsD21NivtOV",
        "Dp6S2emgvrTx",
        "7HijhZDZvzbC"
      ],
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}